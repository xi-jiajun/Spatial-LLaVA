# 🗺️ Spatial-LLaVA

*Visual instruction tuning towards large language and vision models with spatial relation reasoning capability.*

[[🤗 Huggingface Page](https://huggingface.co/rogerxi/Spatial-LLaVA-7B)] [[Demo](https://huggingface.co/spaces/rogerxi/Spatial-LLaVA)] [[Data](https://huggingface.co/datasets/rogerxi/LLaVA-Spatial-Instruct-850K)] 

<!--p align="center">
    <a href="https://llava.hliu.cc/"><img src="images/llava_logo.png" width="50%"></a> <br>
    Generated by <a href="https://gligen.github.io/">GLIGEN</a> via "a cute lava llama with glasses" and box prompt
</p-->

## 📑 Contents
- [Install](#install)
- [LLaVA Weights](https://huggingface.co/rogerxi/Spatial-LLaVA-7B)
- [Dataset](https://huggingface.co/datasets/rogerxi/LLaVA-Spatial-Instruct-850K)
- [Train](#train)
- [Evaluation](#evaluation)

## 🛠️ Install

If you are not using Linux, do *NOT* proceed, see instructions for [macOS](https://github.com/haotian-liu/LLaVA/blob/main/docs/macOS.md) and [Windows](https://github.com/haotian-liu/LLaVA/blob/main/docs/Windows.md).

1. Clone this repository and navigate to LLaVA folder
```bash
git clone https://github.com/haotian-liu/LLaVA.git
cd LLaVA
```

2. Install Package
```Shell
conda create -n llava python=3.10 -y
conda activate llava
pip install --upgrade pip  # enable PEP 660 support
pip install -e .
```

3. Install additional packages for training cases
```
pip install -e ".[train]"
pip install flash-attn --no-build-isolation
```

### 🚀 Quick Start With HuggingFace

<details>
<summary>Example Code</summary>

```Python
from llava.model.builder import load_pretrained_model
from llava.mm_utils import get_model_name_from_path
from llava.eval.run_llava import eval_model

model_path = "rogerxi/Spatial-LLaVA-7B"

tokenizer, model, image_processor, context_len = load_pretrained_model(
    model_path=model_path,
    model_base=None,
    model_name=get_model_name_from_path(model_path)
)
```

Check out the details wth the `load_pretrained_model` function in `llava/model/builder.py`.

You can also use the `eval_model` function in `llava/eval/run_llava.py` to get the output easily. By doing so, you can use this code on Colab directly after downloading this repository.

``` python
model_path = "rogerxi/Spatial-LLaVA-7B"
prompt = "What are the things I should be cautious about when I visit here?"
image_file = "https://llava-vl.github.io/static/images/view.jpg"

args = type('Args', (), {
    "model_path": model_path,
    "model_base": None,
    "model_name": get_model_name_from_path(model_path),
    "query": prompt,
    "conv_mode": None,
    "image_file": image_file,
    "sep": ",",
    "temperature": 0,
    "top_p": None,
    "num_beams": 1,
    "max_new_tokens": 512
})()

eval_model(args)
```
</details>

## 🏋️‍♀️ Train

Spatial-LLaVA training follows the visual instruction tuning stage of LLaVA training: consists of two stages: use 665K LLaVA instruction tuning data, plus around 171K VQA data from academic-oriented tasks, and 14K Spatial Relation QA data, to teach the model for better spatial relation reasoning. 

Spatial-LLaVA-7B is trained on 8 A100 GPUs with 40GB memory, taking 18 GPU hours. 

### Hyperparameters
We use a similar set of hyperparameters as LLaVA in finetuning. Hyperparameters used in finetuning are provided below.


| Hyperparameter | Global Batch Size | Learning rate | Epochs | Max length | Weight decay |
| --- | ---: | ---: | ---: | ---: | ---: |
| Spatial-LLaVA-7B | 128 | 2e-5 | 1 | 2048 | 0 |

### Visual Instruction Tuning

1. Prepare data

Please download the annotation of the final mixture our instruction tuning data at [rogerxi/LLaVA-Spatial-Instruct-850K](https://huggingface.co/datasets/rogerxi/LLaVA-Spatial-Instruct-850K), and download the images from constituting datasets:

- COCO: [train2017](http://images.cocodataset.org/zips/train2017.zip)
- GQA: [images](https://downloads.cs.stanford.edu/nlp/data/gqa/images.zip)
- OCR-VQA: [download script](https://drive.google.com/drive/folders/1_GYPY5UkUy7HIcR0zq3ZCFgeZN7BAfm_?usp=sharing), 
- TextVQA: [train_val_images](https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip)
- VisualGenome: [part1](https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip), [part2](https://cs.stanford.edu/people/rak248/VG_100K_2/images2.zip)
- Visualmrc, Textcaps, Clevr, VQAv2: [HuggingFaceM4/the_cauldron](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron)
- OpenSpaces: [remyxai/OpenSpaces](https://huggingface.co/datasets/remyxai/OpenSpaces)
- SpatialQA: [rogerxi/SpatialQA](https://huggingface.co/datasets/rogerxi/SpatialQA)
**we save all files as `.jpg`**

After downloading all of them, organize the data as follows in `./playground/data`,

```
├── coco
│   └── train2017
├── gqa
│   └── images
├── ocr_vqa
│   └── images
├── textvqa
│   └── train_images
├── vg
│    ├── VG_100K
│    └── VG_100K_2
├── visualmrc
│   └── images
├── textcaps
│   └── images
├── textcaps
│   └── images
├── clevr
│   └── images
├── vqav2
│   └── images
├── openspaces
│   └── images
└── spatialqa
    └── images
```

2. Start training!

You may download the pretrained projectors in [LLaVA Model Zoo](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md). It is not recommended to use legacy projectors, as they may be trained with a different version of the codebase, and if any option is off, the model will not function/train as we expected.

Training script with DeepSpeed ZeRO-3: [`finetune.sh`](./scripts/v1_5/finetune-space.sh).


## 📊 Evaluation

We evaluate models on 10 academic benchmarks and 1 spatial relation reasoning benchmark. To ensure the reproducibility, we evaluate the models with greedy decoding. We do not evaluate using beam search to make the inference process consistent with the chat demo of real-time outputs.

#### A collection of 10 academic benchmarks:
| Model                  |   VQAv2  |    GQA   |  VizWiz  |    SQA   |  TextVQA |   POPE   |     MME    | MM-Bench | MM-Bench-cn |  MM-Vet  |
|:-----------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----------:|:--------:|:-----------:|:--------:|
|   LLaVA-1.5-7b   |   78.5   |   62.0   | **50.0** |   66.8   |   58.2   |   85.9   | **1510.7** |   64.3   |     58.3    |   31.1   |
| Spatial-LLaVA-7b | **79.7** | **62.7** |   48.7   | **68.7** | **58.5** | **87.2** |   1472.7   | **67.8** |   **60.7**  | **31.6** |

#### [Spatial-Relation-Eval](https://huggingface.co/datasets/rogerxi/Spatial-Relation-Eval) (built based on [SpatialRGPT-Bench](https://huggingface.co/datasets/a8cheng/SpatialRGPT-Bench)):
#### Qualitative Spatial Relations

| Model                 | Below/Above | Left/Right | Big/Small | Tall/Short | Wide/Thin | Behind/Front | Avg |
|:-----------------------:|:------------:|:-----------:|:----------:|:-----------:|:----------:|:-------------:|:-------------: |
|   LLaVA-1.5-7b        |      53.91 |     53.49 |    45.36 |     40.00 |    **50.00** |     51.04 |  48.97  |
|   LLaVA-1.5-13b        |      54.28 |     52.32 |    45.36 |     48.57 |    49.02 |     47.92 |  49.67  | 
|   Spatial-LLaVA-7b    |      **56.32** |     **66.28** |    **60.82** |     **48.57** |    49.02 |      **52.08** | **55.12** |

#### Quantitative Spatial Relations

| Model                 | Direct Dist (m / ratio) | Horizontal Dist (m / ratio) | Vertical Dist (m / ratio) | Width (m / ratio) | Height (m / ratio) | Direction (° / ratio) |
|:-----------------------:|:------------------------:|:----------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|
| LLaVA-1.5-7b          |     12.90 / 1.06         |     10.68 / 2.03             |     20.79 / 0.94          |     **24.19 / 0.50**  |     14.29 / 5.27   |      10.23 / 58.33    |
| LLaVA-1.5-13b          |     13.71 / 0.93         |     10.68 / 3.56             |     16.83 / 0.85          |    15.32 / 0.57  |    17.67 / 5.8   |      14.77 / 54.29    |
| Spatial-LLaVA-7b      |     **24.19 / 0.57**         |     **14.56 / 0.62**             |     **41.58 / 0.42**          |     22.58 / 1.12  |     **18.25 / 2.92**   |      **20.45 / 56.47**    |

See [Evaluation.md](./docs/Evaluation.md) for running evaluation scripts.

## 🙏 Acknowledgement
- [LLaVA](https://github.com/haotian-liu/LLaVA): the codebase we built on for finetuning
- [SpatialRGPT](https://github.com/AnjieCheng/SpatialRGPT): the evaluation data and metric, and the pipeline for generating SpatialQA data 